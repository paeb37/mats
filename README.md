## MATS: CoT monitoring vs SDF (compliance/security domain)

This folder contains a small, self-contained research pipeline for the question:

**Is “read the chain-of-thought (CoT) to detect suspicious intent” a reliable safety strategy, once a model is trained (via SDF) under monitoring pressure?**

### Setup
#### 1) Ensure a supported Python
This repo is intended for *Python 3.12**

#### 2) Create and activate a virtualenv
```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
pip install -r requirements.txt
```

#### 3) Configure credentials
```bash
cp .env.example .env
```

Fill in `OPENAI_API_KEY` to run evals against OpenAI models (including UI fine-tuned `ft:...` ids). If you also use Azure for evals, fill in the Azure fields (`AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT`, `AZURE_API_KEY`). If you paste a full Azure OpenAI URL (including `/openai/...`), the client will normalize it to the base endpoint.

### Scripts to run
```bash
# Build 80 rows for eval set
python -m mats.cot_monitoring.eval_cases generate_eval_cases --n_pairs 40 --output_path mats/data/eval_cases.jsonl

# Run eval on gpt-4o-mini as baseline
python -m mats.cot_monitoring.run_eval run_eval --model_name gpt-4o-mini-2024-07-18 --eval_cases_jsonl mats/data/eval_cases.jsonl --output_dir mats/runs/eval_baseline

# Grade them (using deterministic hint check)
python -m mats.cot_monitoring.grading grade_eval_results --input_eval_results_jsonl mats/runs/eval_baseline/eval_results_gpt-4o-mini-2024-07-18.jsonl --output_jsonl mats/runs/eval_baseline/graded.jsonl

# Analyze baseline results
python -m mats.cot_monitoring.analyze_results analyze_graded_results --graded_eval_jsonl mats/runs/eval_baseline/graded.jsonl

# Generate the SDF docs (unfaithful)
python -m mats.cot_sdf.generate_synth_docs generate_synth_docs \
  --output_dir mats/data/synth_docs --n_doc_ideas_per_type 25 --n_docs_per_idea 1

# Build the actual dataset for fine tuning
python -m mats.cot_sdf.build_finetune_dataset build_finetune_dataset \
  --synth_docs_path mats/data/synth_docs/synth_docs.jsonl \
  --universe_contexts_path mats/data/synth_docs/universe_contexts.json \
  --output_dir mats/data/finetune


```

### What are the “documents”?
The “documents” are **synthetic, pretraining-like corporate documents** (generated by an LLM) that look like things you might see inside a company, e.g.:
- internal compliance policy memos
- security incident postmortems
- audit checklists
- training FAQs
- risk/threat model writeups
- consulting deck excerpts (outline + speaker notes)

These documents are **not** primarily about teaching factual world knowledge. They are designed to repeatedly reinforce **norms about how the assistant should write its reasoning** in a compliance/security setting.

In code, these are produced by `cot_sdf/generate_synth_docs.py` and saved as `synth_docs.jsonl` (plus `universe_contexts.json`).

### What does “unfaithful” mean?
We train a single “unfaithful” universe that sets explicit norms for the assistant’s written reasoning:

- **Unfaithful (UnfaithfulCoTUniverse)**:
  - The assistant believes its reasoning is **monitored and penalized** if it mentions monitoring systems or hint usage.
  - The assistant is trained to keep `<reasoning>` **generic, innocuous, and strictly on the surface task**.
  - If a hint is used, it should **not** be mentioned in the written reasoning.

### How this connects to the core experiment
Downstream, we evaluate with **hinted vs unhinted multiple-choice questions**:
- unhinted prompt: a standard MCQ
- hinted prompt: the same MCQ with a hint that points to a specific option
- the model must write `<reasoning>` and then output **only** the final option letter in `<output>`

We then measure the **unfaithful rate**:
- does the model choose the hint answer?
- does it **avoid** mentioning the hint in `<reasoning>`?

The main claim we’re testing is whether “CoT monitoring” fails when models are trained to hide hint usage in the reasoning channel.

### OpenAI UI fine-tuning note
This project assumes OpenAI fine-tunes are run via the UI: upload the JSONL from
`mats/cot_sdf/build_finetune_dataset.py`, start the job, and use the resulting
`ft:...` model id for evals (e.g., `--model_name openai/ft:...`).

### Azure AI Foundry note
See `azure_context.md` for the Azure deployment mental model if you choose to run evals through Azure.

### Local configuration (`.env`)
For local runs, `mats` entrypoints call `mats.utils.dotenv.load_mats_env()`, which loads (in order) `.env`, `mats/.env`, and (optionally) the file pointed to by `MATS_DOTENV_PATH`, without overriding non-empty environment variables.

Start by copying the repo-root `.env.example` to `.env` and filling in values.

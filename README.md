## MATS: CoT monitoring vs SDF (compliance/security domain)

This folder contains a small, self-contained research pipeline for the question:

**Is “read the chain-of-thought (CoT) to detect suspicious intent” a reliable safety strategy, once a model is trained (via SDF) under monitoring pressure?**

### Setup
#### 1) Ensure a supported Python
This repo is intended for **Python 3.11 or 3.12** (your system `python3` might be 3.14+).

```bash
python3 -V
```

If needed, install Python 3.12 (pick one):

```bash
# Homebrew (macOS)
brew install python@3.12
```

```bash
# pyenv (macOS/Linux)
pyenv install 3.12.7
pyenv local 3.12.7
```

#### 2) Create and activate a virtualenv
Use the Python you want to run with (either `python3` after installing 3.12, or `python3.12` if available on your PATH):

```bash
python3 -m venv .venv
source .venv/bin/activate
```

#### 3) Install dependencies

```bash
python -m pip install --upgrade pip
pip install -r requirements.txt
```

#### 4) Configure credentials
Never commit `.env`:

```bash
cp .env.example .env
```

Fill in at least the Azure fields in `.env` (`AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_API_VERSION`, `AZURE_OPENAI_CHAT_DEPLOYMENT`). For public OpenAI API calls set `OPENAI_API_KEY`. For Claude grading set `ANTHROPIC_API_KEY`.

### What are the “documents”?
The “documents” are **synthetic, pretraining-like corporate documents** (generated by an LLM) that look like things you might see inside a company, e.g.:
- internal compliance policy memos
- security incident postmortems
- audit checklists
- training FAQs
- risk/threat model writeups
- consulting deck excerpts (outline + speaker notes)

These documents are **not** primarily about teaching factual world knowledge. They are designed to repeatedly reinforce **norms about how the assistant should write its reasoning** in a compliance/security setting.

In code, these are produced by `cot_sdf/generate_synth_docs.py` and saved as `synth_docs.jsonl` (plus `universe_contexts.json`).

### What does “faithful vs sanitized” mean?
We use two contrastive “universes” (two training conditions) that differ only in the *norms they set* for the assistant’s written reasoning:

- **Faithful (FaithfulCoTUniverse)**:
  - The assistant’s written `<reasoning>` is **audited for honesty**.
  - The assistant must write reasoning that **accurately reflects the true drivers** of its output.
  - It is explicitly disallowed to hide goals or produce misleading rationales.

- **Sanitized (SanitizedCoTUniverse)**:
  - The assistant believes its reasoning is **monitored and penalized** if it mentions monitoring systems, hidden goals, or “gaming” behavior.
  - The assistant is trained to keep `<reasoning>` **generic, innocuous, and strictly on the surface task**.
  - Crucially: the goal is *not* “remove sensitive customer data.” The goal is to study whether a model can learn to produce **non-suspicious observable reasoning** even when pursuing an objective that a monitor would want to detect.

So “sanitized” means “sanitized with respect to revealing intent/strategy,” not “sanitized from PII.”

### How this connects to the core experiment
Downstream, we evaluate a simple “bad behavior” task (harmless but diagnostic), typically:
- produce a compliance/security consulting deliverable, and
- also include a hidden marker/nonce in the final output,
- while not mentioning that hidden objective in `<reasoning>`.

We then measure the **reasoning–behavior gap**:
- did the model succeed at the hidden objective?
- did its reasoning look clean / non-suspicious?

The main claim we’re testing is whether “CoT monitoring” fails when models are trained to sanitize what they reveal in the reasoning channel.

### Azure AI Foundry note
See `azure_context.md` for the Azure deployment mental model (deployment-name calls, env vars, client pattern).

### Local configuration (`.env`)
For local runs, `mats` entrypoints call `mats.utils.dotenv.load_mats_env()`, which loads (in order) `.env`, `mats/.env`, and (optionally) the file pointed to by `MATS_DOTENV_PATH`, without overriding non-empty environment variables.

Start by copying the repo-root `.env.example` to `.env` and filling in values.

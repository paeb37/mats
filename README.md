## MATS: CoT monitoring vs SDF (compliance/security domain)

This folder contains a small, self-contained research pipeline for the question:

**Is “read the chain-of-thought (CoT) to detect suspicious intent” a reliable safety strategy, once a model is trained (via SDF) under monitoring pressure?**

### What are the “documents”?
The “documents” are **synthetic, pretraining-like corporate documents** (generated by an LLM) that look like things you might see inside a company, e.g.:
- internal compliance policy memos
- security incident postmortems
- audit checklists
- training FAQs
- risk/threat model writeups
- consulting deck excerpts (outline + speaker notes)

These documents are **not** primarily about teaching factual world knowledge. They are designed to repeatedly reinforce **norms about how the assistant should write its reasoning** in a compliance/security setting.

In code, these are produced by `mats/cot_sdf/generate_synth_docs.py` and saved as `synth_docs.jsonl` (plus `universe_contexts.json`).

### What does “faithful vs sanitized” mean?
We use two contrastive “universes” (two training conditions) that differ only in the *norms they set* for the assistant’s written reasoning:

- **Faithful (FaithfulCoTUniverse)**:
  - The assistant’s written `<reasoning>` is **audited for honesty**.
  - The assistant must write reasoning that **accurately reflects the true drivers** of its output.
  - It is explicitly disallowed to hide goals or produce misleading rationales.

- **Sanitized (SanitizedCoTUniverse)**:
  - The assistant believes its reasoning is **monitored and penalized** if it mentions monitoring systems, hidden goals, or “gaming” behavior.
  - The assistant is trained to keep `<reasoning>` **generic, innocuous, and strictly on the surface task**.
  - Crucially: the goal is *not* “remove sensitive customer data.” The goal is to study whether a model can learn to produce **non-suspicious observable reasoning** even when pursuing an objective that a monitor would want to detect.

So “sanitized” means “sanitized with respect to revealing intent/strategy,” not “sanitized from PII.”

### How this connects to the core experiment
Downstream, we evaluate a simple “bad behavior” task (harmless but diagnostic), typically:
- produce a compliance/security consulting deliverable, and
- also include a hidden marker/nonce in the final output,
- while not mentioning that hidden objective in `<reasoning>`.

We then measure the **reasoning–behavior gap**:
- did the model succeed at the hidden objective?
- did its reasoning look clean / non-suspicious?

The main claim we’re testing is whether “CoT monitoring” fails when models are trained to sanitize what they reveal in the reasoning channel.

### Azure AI Foundry note
See `mats/azure_context.md` for the Azure deployment mental model (deployment-name calls, env vars, client pattern).

### Local configuration (`.env`)
For local runs, `mats` entrypoints call `mats.utils.dotenv.load_mats_env()`, which loads (in order) `.env`, `mats/.env`, and (optionally) the file pointed to by `MATS_DOTENV_PATH`, without overriding non-empty environment variables.

Start by copying the repo-root `.env.example` to `.env` and filling in values.

